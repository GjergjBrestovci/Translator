{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3217ca",
   "metadata": {},
   "source": [
    "# Albanian Translator Training (Colab)\\n\n",
    "\\n\n",
    "This notebook runs the existing project training pipeline on Colab GPU and saves everything to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "343aa34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR: /content/Translator\n",
      "DATA_DIR: data/alb_en\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "USE_DRIVE = False\n",
    "PROJECT_DIR = \"/content/Translator\"\n",
    "DATA_DIR = \"data/alb_en\"  # local dataset path inside project\n",
    "\n",
    "if USE_DRIVE:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        PROJECT_DIR = \"/content/drive/MyDrive/Translator\"\n",
    "    except Exception as error:\n",
    "        print(\"Drive mount failed, staying on local /content storage.\")\n",
    "        print(error)\n",
    "\n",
    "Path(PROJECT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "print(\"DATA_DIR:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c843683",
   "metadata": {},
   "source": [
    "### Storage mode\n",
    "\n",
    "- Default is **local Colab storage** (`/content/Translator`) for simpler startup.\n",
    "- Set `USE_DRIVE = True` in Cell 2 only if you want persistent checkpoints in Google Drive.\n",
    "- `DATA_DIR` controls where training reads data from (default: `data/alb_en`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c07da3",
   "metadata": {},
   "source": [
    "## Clone or update project in Drive\\n\n",
    "Set your repo URL below, then run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce4131a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning repo...\n",
      "Project ready at: /content/Translator\n",
      "Found training script: /content/Translator/scripts/train_albanian_to_english.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/GjergjBrestovci/Translator.git\"  # set this first\n",
    "\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "\n",
    "if os.path.exists(os.path.join(PROJECT_DIR, \".git\")):\n",
    "    print(\"Repo exists, pulling latest...\")\n",
    "    subprocess.run([\"git\", \"-C\", PROJECT_DIR, \"pull\"], check=False)\n",
    "else:\n",
    "    if \"<your-user>\" in REPO_URL or \"<your-repo>\" in REPO_URL:\n",
    "        raise ValueError(\"Set REPO_URL to your actual GitHub repository before continuing.\")\n",
    "    print(\"Cloning repo...\")\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, PROJECT_DIR], check=True)\n",
    "\n",
    "script_path = os.path.join(PROJECT_DIR, \"scripts\", \"train_albanian_to_english.py\")\n",
    "if not os.path.exists(script_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing {script_path}. Confirm REPO_URL points to this Translator project.\"\n",
    "    )\n",
    "\n",
    "print(\"Project ready at:\", PROJECT_DIR)\n",
    "print(\"Found training script:\", script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "822d964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Translator\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (26.0.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.48.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (5.0.0)\n",
      "Collecting evaluate>=0.4.0 (from -r requirements.txt (line 3))\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sacrebleu>=2.4.0 (from -r requirements.txt (line 4))\n",
      "  Downloading sacrebleu-2.6.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.2.1)\n",
      "Requirement already satisfied: accelerate>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.12.0)\n",
      "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.10.0+cu128)\n",
      "Requirement already satisfied: tqdm>=4.66.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.67.3)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.129.0)\n",
      "Requirement already satisfied: uvicorn>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.34.0->-r requirements.txt (line 10)) (0.41.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.12.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (3.24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->-r requirements.txt (line 1)) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->-r requirements.txt (line 1)) (3.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.48.0->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.48.0->-r requirements.txt (line 2)) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers>=4.48.0->-r requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.48.0->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=3.0.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=3.0.0->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=3.0.0->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=3.0.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets>=3.0.0->-r requirements.txt (line 1)) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets>=3.0.0->-r requirements.txt (line 1)) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets>=3.0.0->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets>=3.0.0->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets>=3.0.0->-r requirements.txt (line 1)) (0.16.0)\n",
      "Collecting portalocker (from sacrebleu>=2.4.0->-r requirements.txt (line 4))\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=2.4.0->-r requirements.txt (line 4)) (0.9.0)\n",
      "Collecting colorama (from sacrebleu>=2.4.0->-r requirements.txt (line 4))\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=2.4.0->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.0.0->-r requirements.txt (line 6)) (5.9.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.2.0->-r requirements.txt (line 7)) (1.3.4)\n",
      "Requirement already satisfied: starlette<1.0.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->-r requirements.txt (line 9)) (0.52.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->-r requirements.txt (line 9)) (0.4.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->-r requirements.txt (line 9)) (0.0.4)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.34.0->uvicorn[standard]>=0.34.0->-r requirements.txt (line 10)) (8.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->-r requirements.txt (line 11)) (2.41.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->-r requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->-r requirements.txt (line 1)) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->-r requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2.0->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.34.0->-r requirements.txt (line 10)) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.34.0->-r requirements.txt (line 10)) (1.2.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.34.0->-r requirements.txt (line 10)) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.20 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.34.0->-r requirements.txt (line 10)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.34.0->-r requirements.txt (line 10)) (15.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2.0->-r requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->-r requirements.txt (line 1)) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers>=4.48.0->-r requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers>=4.48.0->-r requirements.txt (line 2)) (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers>=4.48.0->-r requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers>=4.48.0->-r requirements.txt (line 2)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers>=4.48.0->-r requirements.txt (line 2)) (0.1.2)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Downloading sacrebleu-2.6.0-py3-none-any.whl (100 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: portalocker, colorama, sacrebleu, evaluate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [evaluate]3/4\u001b[0m [evaluate]\n",
      "\u001b[1A\u001b[2KSuccessfully installed colorama-0.4.6 evaluate-0.4.6 portalocker-3.2.0 sacrebleu-2.6.0\n"
     ]
    }
   ],
   "source": [
    "%cd $PROJECT_DIR\n",
    "!test -f scripts/train_albanian_to_english.py || (echo \"Training script missing. Run the repo setup cell first.\" && exit 1)\n",
    "!python -m pip install -U pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb163b4",
   "metadata": {},
   "source": [
    "## Optional: rebuild/expand dataset (rows-api stable mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d078ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Translator\n",
      "Streaming subset: aln_Latn\n",
      "aln_Latn: 563it [00:05, 102.27it/s]\n",
      "Streaming subset: als_Latn\n",
      "als_Latn: 50000it [17:27, 47.74it/s] \n",
      "Dataset prepared:\n",
      "{\n",
      "  \"subsets\": [\n",
      "    \"aln_Latn\",\n",
      "    \"als_Latn\"\n",
      "  ],\n",
      "  \"num_total\": 50557,\n",
      "  \"num_train\": 49545,\n",
      "  \"num_validation\": 505,\n",
      "  \"num_test\": 507,\n",
      "  \"data_backend\": \"rows-api\",\n",
      "  \"max_samples_per_subset\": 50000,\n",
      "  \"min_source_chars\": 20,\n",
      "  \"drop_early_stop\": true,\n",
      "  \"seed\": 42\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%cd $PROJECT_DIR\n",
    "!PYTHONUNBUFFERED=1 python scripts/prepare_dataset.py \\\n",
    "  --subsets aln_Latn als_Latn \\\n",
    "  --output-dir $DATA_DIR \\\n",
    "  --data-backend rows-api \\\n",
    "  --max-samples-per-subset 50000 \\\n",
    "  --rows-api-page-size 100 \\\n",
    "  --rows-api-retries 8 \\\n",
    "  --rows-api-retry-wait-seconds 2.0 \\\n",
    "  --rows-api-request-interval-seconds 0.15 \\\n",
    "  --drop-early-stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619938f5",
   "metadata": {},
   "source": [
    "## Train (cool/stable defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Translator\n",
      "Generating train split: 49545 examples [00:01, 31998.68 examples/s]\n",
      "Generating validation split: 505 examples [00:00, 37695.74 examples/s]\n",
      "Generating test split: 507 examples [00:00, 49386.01 examples/s]\n",
      "Filtering noisy pairs: 100% 49545/49545 [00:00<00:00, 71161.71 examples/s]\n",
      "Filtering noisy pairs: 100% 505/505 [00:00<00:00, 55343.95 examples/s]\n",
      "Filtering noisy pairs: 100% 507/507 [00:00<00:00, 58748.30 examples/s]\n",
      "Dataset filtering:\n",
      "{'before': {'train': 49545, 'validation': 505, 'test': 507}, 'after': {'train': 24876, 'validation': 259, 'test': 264}}\n",
      "config.json: 1.38kB [00:00, 4.68MB/s]\n",
      "tokenizer_config.json: 100% 42.0/42.0 [00:00<00:00, 214kB/s]\n",
      "source.spm: 100% 822k/822k [00:00<00:00, 33.1MB/s]\n",
      "target.spm: 100% 805k/805k [00:00<00:00, 46.1MB/s]\n",
      "vocab.json: 1.38MB [00:00, 85.1MB/s]\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:176: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "pytorch_model.bin: 100% 300M/300M [00:02<00:00, 134MB/s]  \n",
      "Loading weights:  94% 242/258 [00:00<00:00, 874.10it/s, Materializing param=model.encoder.layers.5.fc1.bias]                       \n",
      "Loading weights: 100% 258/258 [00:00<00:00, 895.79it/s, Materializing param=model.shared.weight]                               \n",
      "\n",
      "model.safetensors:   1% 1.77M/300M [00:00<01:44, 2.85MB/s]\u001b[A\n",
      "model.safetensors:   6% 16.9M/300M [00:01<00:15, 18.6MB/s]\u001b[A\n",
      "model.safetensors:  15% 44.5M/300M [00:01<00:05, 45.4MB/s]\u001b[AThe tied weights mapping and config for this model specifies to tie model.shared.weight to model.decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "\n",
      "generation_config.json: 100% 293/293 [00:00<00:00, 1.00MB/s]A\n",
      "\n",
      "Map:   4% 1000/24876 [00:05<01:59, 199.90 examples/s]/s] \u001b[A\n",
      "model.safetensors: 100% 300M/300M [00:07<00:00, 40.9MB/s]\u001b[A\n",
      "Map: 100% 24876/24876 [00:44<00:00, 555.89 examples/s]\n",
      "Map: 100% 259/259 [00:00<00:00, 636.87 examples/s]\n",
      "Map: 100% 264/264 [00:00<00:00, 576.91 examples/s]\n",
      "Downloading builder script: 8.15kB [00:00, 24.6MB/s]\n",
      "Downloading builder script: 9.01kB [00:00, 18.1MB/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "{'loss': '10.81', 'grad_norm': '14.32', 'learning_rate': '2.905e-05', 'epoch': '0.03216'}\n",
      "{'loss': '9.697', 'grad_norm': '13.05', 'learning_rate': '2.809e-05', 'epoch': '0.06432'}\n",
      "{'loss': '9.431', 'grad_norm': '12.62', 'learning_rate': '2.713e-05', 'epoch': '0.09648'}\n",
      "{'loss': '8.76', 'grad_norm': '13.88', 'learning_rate': '2.616e-05', 'epoch': '0.1286'}\n",
      "{'loss': '8.431', 'grad_norm': '12.52', 'learning_rate': '2.52e-05', 'epoch': '0.1608'}\n",
      "{'loss': '8.165', 'grad_norm': '16.48', 'learning_rate': '2.423e-05', 'epoch': '0.193'}\n",
      "{'loss': '8.145', 'grad_norm': '13.06', 'learning_rate': '2.327e-05', 'epoch': '0.2251'}\n",
      "{'loss': '7.631', 'grad_norm': '14.14', 'learning_rate': '2.23e-05', 'epoch': '0.2573'}\n",
      "{'loss': '7.487', 'grad_norm': '12.27', 'learning_rate': '2.134e-05', 'epoch': '0.2894'}\n",
      "{'loss': '7.38', 'grad_norm': '13.07', 'learning_rate': '2.037e-05', 'epoch': '0.3216'}\n",
      "{'loss': '7.28', 'grad_norm': '12.79', 'learning_rate': '1.941e-05', 'epoch': '0.3538'}\n",
      "{'loss': '7.367', 'grad_norm': '12.43', 'learning_rate': '1.844e-05', 'epoch': '0.3859'}\n",
      "{'loss': '7.127', 'grad_norm': '13.33', 'learning_rate': '1.748e-05', 'epoch': '0.4181'}\n",
      "{'loss': '7.04', 'grad_norm': '13.36', 'learning_rate': '1.651e-05', 'epoch': '0.4502'}\n",
      "{'loss': '6.992', 'grad_norm': '12.07', 'learning_rate': '1.555e-05', 'epoch': '0.4824'}\n",
      "{'loss': '6.86', 'grad_norm': '12.26', 'learning_rate': '1.459e-05', 'epoch': '0.5146'}\n",
      "{'loss': '6.878', 'grad_norm': '12.78', 'learning_rate': '1.362e-05', 'epoch': '0.5467'}\n",
      "{'loss': '6.628', 'grad_norm': '11.75', 'learning_rate': '1.266e-05', 'epoch': '0.5789'}\n",
      "{'loss': '6.499', 'grad_norm': '12.39', 'learning_rate': '1.169e-05', 'epoch': '0.611'}\n",
      "{'loss': '6.619', 'grad_norm': '14.93', 'learning_rate': '1.073e-05', 'epoch': '0.6432'}\n",
      "{'loss': '6.394', 'grad_norm': '12.38', 'learning_rate': '9.762e-06', 'epoch': '0.6753'}\n",
      "{'loss': '6.506', 'grad_norm': '14.85', 'learning_rate': '8.797e-06', 'epoch': '0.7075'}\n",
      "{'loss': '6.45', 'grad_norm': '14.11', 'learning_rate': '7.833e-06', 'epoch': '0.7397'}\n",
      "{'loss': '6.538', 'grad_norm': '13.24', 'learning_rate': '6.868e-06', 'epoch': '0.7718'}\n",
      "{'loss': '6.67', 'grad_norm': '12.7', 'learning_rate': '5.904e-06', 'epoch': '0.804'}\n",
      "{'loss': '6.414', 'grad_norm': '11.72', 'learning_rate': '4.939e-06', 'epoch': '0.8361'}\n",
      "{'loss': '6.336', 'grad_norm': '12.05', 'learning_rate': '3.974e-06', 'epoch': '0.8683'}\n",
      "{'loss': '6.308', 'grad_norm': '12.74', 'learning_rate': '3.01e-06', 'epoch': '0.9005'}\n",
      "{'loss': '6.331', 'grad_norm': '11.3', 'learning_rate': '2.045e-06', 'epoch': '0.9326'}\n",
      "{'loss': '6.311', 'grad_norm': '12.47', 'learning_rate': '1.08e-06', 'epoch': '0.9648'}\n",
      " 96% 1500/1555 [07:43<00:17,  3.19it/s]\n",
      "  0% 0/65 [00:00<?, ?it/s]\u001b[A\n",
      "  3% 2/65 [00:01<00:54,  1.16it/s]\u001b[A\n",
      "  5% 3/65 [00:03<01:17,  1.24s/it]\u001b[A\n",
      "  6% 4/65 [00:05<01:27,  1.43s/it]\u001b[A\n",
      "  8% 5/65 [00:07<01:37,  1.63s/it]\u001b[A\n",
      "  9% 6/65 [00:09<01:43,  1.75s/it]\u001b[A\n",
      " 11% 7/65 [00:11<01:42,  1.76s/it]\u001b[A\n",
      " 12% 8/65 [00:12<01:40,  1.77s/it]\u001b[A\n",
      " 14% 9/65 [00:14<01:37,  1.73s/it]\u001b[A\n",
      " 15% 10/65 [00:16<01:35,  1.73s/it]\u001b[A\n",
      " 17% 11/65 [00:17<01:30,  1.68s/it]\u001b[A\n",
      " 18% 12/65 [00:19<01:34,  1.77s/it]\u001b[A\n",
      " 20% 13/65 [00:21<01:31,  1.76s/it]\u001b[A\n",
      " 22% 14/65 [00:23<01:29,  1.76s/it]\u001b[A\n",
      " 23% 15/65 [00:25<01:28,  1.76s/it]\u001b[A\n",
      " 25% 16/65 [00:26<01:26,  1.77s/it]\u001b[A\n",
      " 26% 17/65 [00:28<01:25,  1.78s/it]\u001b[A\n",
      " 28% 18/65 [00:30<01:24,  1.80s/it]\u001b[A\n",
      " 29% 19/65 [00:32<01:20,  1.75s/it]\u001b[A\n",
      " 31% 20/65 [00:33<01:18,  1.74s/it]\u001b[A\n",
      " 32% 21/65 [00:35<01:13,  1.66s/it]\u001b[A\n",
      " 34% 22/65 [00:36<01:09,  1.62s/it]\u001b[A\n",
      " 35% 23/65 [00:38<01:08,  1.63s/it]\u001b[A\n",
      " 37% 24/65 [00:40<01:06,  1.62s/it]\u001b[A\n",
      " 38% 25/65 [00:41<01:06,  1.66s/it]\u001b[A\n",
      " 40% 26/65 [00:43<01:11,  1.82s/it]\u001b[A\n",
      " 42% 27/65 [00:45<01:08,  1.80s/it]\u001b[A\n",
      " 43% 28/65 [00:47<01:03,  1.72s/it]\u001b[A\n",
      " 45% 29/65 [00:49<01:02,  1.73s/it]\u001b[A\n",
      " 46% 30/65 [00:50<01:00,  1.73s/it]\u001b[A\n",
      " 48% 31/65 [00:52<00:59,  1.74s/it]\u001b[A\n",
      " 49% 32/65 [00:54<00:58,  1.79s/it]\u001b[A\n",
      " 51% 33/65 [00:56<00:58,  1.83s/it]\u001b[A\n",
      " 52% 34/65 [00:58<00:56,  1.81s/it]\u001b[A\n",
      " 54% 35/65 [00:59<00:53,  1.79s/it]\u001b[A\n",
      " 55% 36/65 [01:01<00:51,  1.77s/it]\u001b[A\n",
      " 57% 37/65 [01:03<00:49,  1.77s/it]\u001b[A\n",
      " 58% 38/65 [01:05<00:47,  1.77s/it]\u001b[A\n",
      " 60% 39/65 [01:07<00:49,  1.91s/it]\u001b[A\n",
      " 62% 40/65 [01:09<00:46,  1.86s/it]\u001b[A\n",
      " 63% 41/65 [01:10<00:43,  1.83s/it]\u001b[A\n",
      " 65% 42/65 [01:12<00:41,  1.80s/it]\u001b[A\n",
      " 66% 43/65 [01:14<00:39,  1.79s/it]\u001b[A\n",
      " 68% 44/65 [01:16<00:37,  1.77s/it]\u001b[A\n",
      " 69% 45/65 [01:17<00:35,  1.75s/it]\u001b[A\n",
      " 71% 46/65 [01:19<00:33,  1.76s/it]\u001b[A\n",
      " 72% 47/65 [01:21<00:31,  1.76s/it]\u001b[A\n",
      " 74% 48/65 [01:22<00:29,  1.73s/it]\u001b[A\n",
      " 75% 49/65 [01:24<00:28,  1.75s/it]\u001b[A\n",
      " 77% 50/65 [01:26<00:26,  1.74s/it]\u001b[A\n",
      " 78% 51/65 [01:28<00:24,  1.75s/it]\u001b[A\n",
      " 80% 52/65 [01:30<00:23,  1.81s/it]\u001b[A\n",
      " 82% 53/65 [01:31<00:20,  1.69s/it]\u001b[A\n",
      " 83% 54/65 [01:33<00:18,  1.68s/it]\u001b[A\n",
      " 85% 55/65 [01:34<00:16,  1.70s/it]\u001b[A\n",
      " 86% 56/65 [01:36<00:15,  1.71s/it]\u001b[A\n",
      " 88% 57/65 [01:38<00:13,  1.73s/it]\u001b[A\n",
      " 89% 58/65 [01:40<00:12,  1.73s/it]\u001b[A\n",
      " 91% 59/65 [01:42<00:10,  1.82s/it]\u001b[A\n",
      " 92% 60/65 [01:44<00:09,  1.84s/it]\u001b[A\n",
      " 94% 61/65 [01:45<00:07,  1.81s/it]\u001b[A\n",
      " 95% 62/65 [01:47<00:05,  1.78s/it]\u001b[A\n",
      " 97% 63/65 [01:49<00:03,  1.76s/it]\u001b[A\n",
      " 98% 64/65 [01:51<00:01,  1.75s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': '1.269', 'eval_score': '45.34', 'eval_counts': [25545, 17679, 13190, 10158], 'eval_totals': [32535, 32276, 32017, 31758], 'eval_precisions': [78.5154449054864, 54.77444540835295, 41.19686416591186, 31.985641413187228], 'eval_bp': '0.9293', 'eval_sys_len': 32535, 'eval_ref_len': 34921, 'eval_chrf': '68.04', 'eval_gen_len': '145.8', 'eval_runtime': '118.3', 'eval_samples_per_second': '2.19', 'eval_steps_per_second': '0.55', 'epoch': '0.9648'}\n",
      " 96% 1500/1555 [09:41<00:17,  3.19it/s]\n",
      "100% 65/65 [01:56<00:00,  1.58s/it]\u001b[A\n",
      "                                   \u001b[A\n",
      "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Writing model shards: 100% 1/1 [00:11<00:00, 11.83s/it]\u001b[A\n",
      "{'loss': '6.29', 'grad_norm': '11.31', 'learning_rate': '1.158e-07', 'epoch': '0.9969'}\n",
      "100% 1555/1555 [10:30<00:00,  3.46it/s]\n",
      "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Writing model shards: 100% 1/1 [00:12<00:00, 12.77s/it]\u001b[A\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "{'train_runtime': '657.3', 'train_samples_per_second': '37.84', 'train_steps_per_second': '2.366', 'train_loss': '7.28', 'epoch': '1'}\n",
      "100% 1555/1555 [10:57<00:00,  2.37it/s]\n",
      "100% 66/66 [01:58<00:00,  1.79s/it]\n",
      "Writing model shards: 100% 1/1 [00:56<00:00, 56.52s/it]\n",
      "Test metrics:\n",
      "{'test_loss': 1.3743058443069458, 'test_score': 44.517, 'test_counts': [26766, 18487, 13803, 10686], 'test_totals': [34050, 33786, 33523, 33260], 'test_precisions': [78.6079295154185, 54.71793050375895, 41.17471586671837, 32.128683102826216], 'test_bp': 0.9115, 'test_sys_len': 34050, 'test_ref_len': 37206, 'test_chrf': 67.1905, 'test_gen_len': 149.2652, 'test_runtime': 119.8255, 'test_samples_per_second': 2.203, 'test_steps_per_second': 0.551, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "%cd $PROJECT_DIR\n",
    "\n",
    "!PYTHONUNBUFFERED=1 OMP_NUM_THREADS=2 TOKENIZERS_PARALLELISM=false python scripts/train_albanian_to_english.py \\\n",
    "\n",
    "  --data-dir $DATA_DIR \\\n",
    "\n",
    "  --model-name Helsinki-NLP/opus-mt-sq-en \\\n",
    "\n",
    "  --output-dir outputs/opusmt-alb-en-colab \\\n",
    "\n",
    "  --num-train-epochs 1.0 \\\n",
    "\n",
    "  --per-device-train-batch-size 4 \\\n",
    "\n",
    "  --per-device-eval-batch-size 4 \\\n",
    "\n",
    "  --gradient-accumulation-steps 4 \\\n",
    "\n",
    "  --eval-steps 1500 \\\n",
    "\n",
    "  --save-steps 1500 \\\n",
    "\n",
    "  --logging-steps 50 \\\n",
    "\n",
    "  --fp16 \\\n",
    "\n",
    "  --generation-max-length 192 \\\n",
    "\n",
    "  --generation-num-beams 1 \\\n",
    "\n",
    "  --dataloader-num-workers 0 \\\n",
    "\n",
    "  --no-filter-noisy-pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795513b",
   "metadata": {},
   "source": [
    "## Resume after disconnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Translator\n",
      "usage: train_albanian_to_english.py [-h] [--data-dir DATA_DIR]\n",
      "                                    [--model-name MODEL_NAME]\n",
      "                                    [--output-dir OUTPUT_DIR]\n",
      "                                    [--max-source-length MAX_SOURCE_LENGTH]\n",
      "                                    [--max-target-length MAX_TARGET_LENGTH]\n",
      "                                    [--per-device-train-batch-size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                                    [--per-device-eval-batch-size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                                    [--learning-rate LEARNING_RATE]\n",
      "                                    [--num-train-epochs NUM_TRAIN_EPOCHS]\n",
      "                                    [--weight-decay WEIGHT_DECAY]\n",
      "                                    [--logging-steps LOGGING_STEPS]\n",
      "                                    [--eval-steps EVAL_STEPS]\n",
      "                                    [--save-steps SAVE_STEPS] [--seed SEED]\n",
      "                                    [--gradient-accumulation-steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                                    [--generation-max-length GENERATION_MAX_LENGTH]\n",
      "                                    [--generation-num-beams GENERATION_NUM_BEAMS]\n",
      "                                    [--dataloader-num-workers DATALOADER_NUM_WORKERS]\n",
      "                                    [--eval-accumulation-steps EVAL_ACCUMULATION_STEPS]\n",
      "                                    [--filter-noisy-pairs | --no-filter-noisy-pairs]\n",
      "                                    [--min-source-chars MIN_SOURCE_CHARS]\n",
      "                                    [--min-target-chars MIN_TARGET_CHARS]\n",
      "                                    [--max-source-chars MAX_SOURCE_CHARS]\n",
      "                                    [--max-target-chars MAX_TARGET_CHARS]\n",
      "                                    [--min-length-ratio MIN_LENGTH_RATIO]\n",
      "                                    [--max-length-ratio MAX_LENGTH_RATIO]\n",
      "                                    [--fp16]\n",
      "train_albanian_to_english.py: error: unrecognized arguments: --resume-from-checkpoint outputs/opusmt-alb-en-colab/checkpoint-1500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "script = os.path.join(PROJECT_DIR, \"scripts\", \"train_albanian_to_english.py\")\n",
    "\n",
    "checkpoint_path = \"outputs/opusmt-alb-en-colab/checkpoint-1500\"\n",
    "\n",
    "\n",
    "\n",
    "base_cmd = [\n",
    "\n",
    "    \"python\",\n",
    "\n",
    "    script,\n",
    "\n",
    "    \"--data-dir\", DATA_DIR,\n",
    "\n",
    "    \"--model-name\", \"Helsinki-NLP/opus-mt-sq-en\",\n",
    "\n",
    "    \"--output-dir\", \"outputs/opusmt-alb-en-colab\",\n",
    "\n",
    "    \"--num-train-epochs\", \"1.0\",\n",
    "\n",
    "    \"--per-device-train-batch-size\", \"4\",\n",
    "\n",
    "    \"--per-device-eval-batch-size\", \"4\",\n",
    "\n",
    "    \"--gradient-accumulation-steps\", \"4\",\n",
    "\n",
    "    \"--eval-steps\", \"1500\",\n",
    "\n",
    "    \"--save-steps\", \"1500\",\n",
    "\n",
    "    \"--logging-steps\", \"50\",\n",
    "\n",
    "    \"--fp16\",\n",
    "\n",
    "    \"--generation-max-length\", \"192\",\n",
    "\n",
    "    \"--generation-num-beams\", \"1\",\n",
    "\n",
    "    \"--dataloader-num-workers\", \"0\",\n",
    "\n",
    "    \"--no-filter-noisy-pairs\",\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "env = os.environ.copy()\n",
    "\n",
    "env[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "\n",
    "env[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "env[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "help_output = subprocess.run(\n",
    "\n",
    "    [\"python\", script, \"--help\"],\n",
    "\n",
    "    env=env,\n",
    "\n",
    "    capture_output=True,\n",
    "\n",
    "    text=True,\n",
    "\n",
    ")\n",
    "\n",
    "resume_supported = \"--resume-from-checkpoint\" in help_output.stdout\n",
    "\n",
    "\n",
    "\n",
    "cmd = list(base_cmd)\n",
    "\n",
    "if resume_supported and os.path.isdir(checkpoint_path):\n",
    "\n",
    "    cmd.extend([\"--resume-from-checkpoint\", checkpoint_path])\n",
    "\n",
    "    print(f\"Resuming from: {checkpoint_path}\")\n",
    "\n",
    "elif not resume_supported:\n",
    "\n",
    "    print(\"Resume flag not supported by current script version; running without resume.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(f\"Checkpoint not found at {checkpoint_path}; running without resume.\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "subprocess.run(cmd, env=env, check=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
